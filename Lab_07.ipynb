{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate 조절 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training our model using this dataset\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.46245\n",
      "1 1.67474\n",
      "2 1.01396\n",
      "3 0.889447\n",
      "4 0.869322\n",
      "5 0.860114\n",
      "6 0.852392\n",
      "7 0.846404\n",
      "8 0.841017\n",
      "9 0.836299\n",
      "10 0.831888\n",
      "11 0.827795\n",
      "12 0.823885\n",
      "13 0.820154\n",
      "14 0.816545\n",
      "15 0.813053\n",
      "16 0.809653\n",
      "17 0.806338\n",
      "18 0.803097\n",
      "19 0.799926\n",
      "20 0.796819\n",
      "21 0.793773\n",
      "22 0.790783\n",
      "23 0.787847\n",
      "24 0.784964\n",
      "25 0.782131\n",
      "26 0.779346\n",
      "27 0.776608\n",
      "28 0.773915\n",
      "29 0.771267\n",
      "30 0.768661\n",
      "31 0.766096\n",
      "32 0.763572\n",
      "33 0.761087\n",
      "34 0.758641\n",
      "35 0.756232\n",
      "36 0.753859\n",
      "37 0.751521\n",
      "38 0.749218\n",
      "39 0.746949\n",
      "40 0.744712\n",
      "41 0.742508\n",
      "42 0.740334\n",
      "43 0.738192\n",
      "44 0.736079\n",
      "45 0.733995\n",
      "46 0.731939\n",
      "47 0.729911\n",
      "48 0.727911\n",
      "49 0.725936\n",
      "50 0.723988\n",
      "51 0.722065\n",
      "52 0.720167\n",
      "53 0.718293\n",
      "54 0.716443\n",
      "55 0.714616\n",
      "56 0.712811\n",
      "57 0.711029\n",
      "58 0.709268\n",
      "59 0.707529\n",
      "60 0.705811\n",
      "61 0.704113\n",
      "62 0.702435\n",
      "63 0.700776\n",
      "64 0.699137\n",
      "65 0.697517\n",
      "66 0.695914\n",
      "67 0.69433\n",
      "68 0.692764\n",
      "69 0.691215\n",
      "70 0.689683\n",
      "71 0.688168\n",
      "72 0.686669\n",
      "73 0.685187\n",
      "74 0.68372\n",
      "75 0.682268\n",
      "76 0.680832\n",
      "77 0.67941\n",
      "78 0.678003\n",
      "79 0.676611\n",
      "80 0.675232\n",
      "81 0.673868\n",
      "82 0.672517\n",
      "83 0.671179\n",
      "84 0.669855\n",
      "85 0.668544\n",
      "86 0.667245\n",
      "87 0.665958\n",
      "88 0.664684\n",
      "89 0.663422\n",
      "90 0.662171\n",
      "91 0.660933\n",
      "92 0.659705\n",
      "93 0.658489\n",
      "94 0.657284\n",
      "95 0.65609\n",
      "96 0.654907\n",
      "97 0.653734\n",
      "98 0.652571\n",
      "99 0.651419\n",
      "100 0.650276\n",
      "101 0.649144\n",
      "102 0.648021\n",
      "103 0.646908\n",
      "104 0.645804\n",
      "105 0.644709\n",
      "106 0.643624\n",
      "107 0.642547\n",
      "108 0.641479\n",
      "109 0.64042\n",
      "110 0.63937\n",
      "111 0.638327\n",
      "112 0.637294\n",
      "113 0.636268\n",
      "114 0.63525\n",
      "115 0.634241\n",
      "116 0.633239\n",
      "117 0.632245\n",
      "118 0.631258\n",
      "119 0.63028\n",
      "120 0.629308\n",
      "121 0.628343\n",
      "122 0.627386\n",
      "123 0.626436\n",
      "124 0.625493\n",
      "125 0.624557\n",
      "126 0.623627\n",
      "127 0.622704\n",
      "128 0.621788\n",
      "129 0.620878\n",
      "130 0.619975\n",
      "131 0.619078\n",
      "132 0.618187\n",
      "133 0.617302\n",
      "134 0.616424\n",
      "135 0.615551\n",
      "136 0.614684\n",
      "137 0.613824\n",
      "138 0.612968\n",
      "139 0.612119\n",
      "140 0.611275\n",
      "141 0.610437\n",
      "142 0.609604\n",
      "143 0.608776\n",
      "144 0.607954\n",
      "145 0.607137\n",
      "146 0.606325\n",
      "147 0.605519\n",
      "148 0.604717\n",
      "149 0.603921\n",
      "150 0.603129\n",
      "151 0.602342\n",
      "152 0.60156\n",
      "153 0.600783\n",
      "154 0.60001\n",
      "155 0.599242\n",
      "156 0.598479\n",
      "157 0.59772\n",
      "158 0.596966\n",
      "159 0.596216\n",
      "160 0.59547\n",
      "161 0.594729\n",
      "162 0.593992\n",
      "163 0.593259\n",
      "164 0.59253\n",
      "165 0.591806\n",
      "166 0.591085\n",
      "167 0.590369\n",
      "168 0.589656\n",
      "169 0.588947\n",
      "170 0.588243\n",
      "171 0.587542\n",
      "172 0.586845\n",
      "173 0.586151\n",
      "174 0.585462\n",
      "175 0.584776\n",
      "176 0.584093\n",
      "177 0.583414\n",
      "178 0.582739\n",
      "179 0.582068\n",
      "180 0.581399\n",
      "181 0.580734\n",
      "182 0.580073\n",
      "183 0.579415\n",
      "184 0.57876\n",
      "185 0.578109\n",
      "186 0.577461\n",
      "187 0.576816\n",
      "188 0.576174\n",
      "189 0.575535\n",
      "190 0.5749\n",
      "191 0.574267\n",
      "192 0.573638\n",
      "193 0.573012\n",
      "194 0.572388\n",
      "195 0.571768\n",
      "196 0.57115\n",
      "197 0.570536\n",
      "198 0.569924\n",
      "199 0.569315\n",
      "200 0.568709\n",
      "Prediction: [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    #training process is based on training data\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val)\n",
    "    \n",
    "    # Evaluation is based on test data\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data normalization 실습 without MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  9.20215e+10 \n",
      "Prediction:\n",
      " [[ 214129.453125]\n",
      " [ 432396.75    ]\n",
      " [ 339879.375   ]\n",
      " [ 237873.015625]\n",
      " [ 280571.625   ]\n",
      " [ 282945.78125 ]\n",
      " [ 259233.171875]\n",
      " [ 330407.125   ]]\n",
      "1 Cost:  1.01102e+26 \n",
      "Prediction:\n",
      " [[ -7.09269284e+12]\n",
      " [ -1.42783178e+13]\n",
      " [ -1.12322381e+13]\n",
      " [ -7.87373883e+12]\n",
      " [ -9.27962287e+12]\n",
      " [ -9.35772711e+12]\n",
      " [ -8.57668007e+12]\n",
      " [ -1.09198191e+13]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[  2.35096890e+20]\n",
      " [  4.73274137e+20]\n",
      " [  3.72307691e+20]\n",
      " [  2.60985692e+20]\n",
      " [  3.07585616e+20]\n",
      " [  3.10174517e+20]\n",
      " [  2.84285645e+20]\n",
      " [  3.61952156e+20]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[ -7.79260243e+27]\n",
      " [ -1.56873083e+28]\n",
      " [ -1.23406392e+28]\n",
      " [ -8.65072253e+27]\n",
      " [ -1.01953390e+28]\n",
      " [ -1.02811514e+28]\n",
      " [ -9.42303074e+27]\n",
      " [ -1.19973916e+28]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[  2.58296308e+35]\n",
      " [  5.19976966e+35]\n",
      " [  4.09047120e+35]\n",
      " [  2.86739872e+35]\n",
      " [  3.37938259e+35]\n",
      " [  3.40782590e+35]\n",
      " [  3.12339066e+35]\n",
      " [  3.97669718e+35]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        cost_val, hy_val, _ = sess.run(\n",
    "            [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data normalization with MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99999999  0.99999999  0.          1.          1.        ]\n",
      " [ 0.70548491  0.70439552  1.          0.71881782  0.83755791]\n",
      " [ 0.54412549  0.50274824  0.57608696  0.606468    0.6606331 ]\n",
      " [ 0.33890353  0.31368023  0.10869565  0.45989134  0.43800918]\n",
      " [ 0.51436     0.42582389  0.30434783  0.58504805  0.42624401]\n",
      " [ 0.49556179  0.42582389  0.31521739  0.48131134  0.49276137]\n",
      " [ 0.11436064  0.          0.20652174  0.22007776  0.18597238]\n",
      " [ 0.          0.07747099  0.5326087   0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "xy = MinMaxScaler(xy)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  1.09015 \n",
      "Prediction:\n",
      " [[-0.309044  ]\n",
      " [-1.02813196]\n",
      " [-0.57128328]\n",
      " [-0.08181371]\n",
      " [-0.3141489 ]\n",
      " [-0.42486733]\n",
      " [-0.08411439]\n",
      " [-0.52518469]]\n",
      "1 Cost:  1.09008 \n",
      "Prediction:\n",
      " [[-0.30899292]\n",
      " [-1.02808166]\n",
      " [-0.57124168]\n",
      " [-0.08178209]\n",
      " [-0.31411123]\n",
      " [-0.42483091]\n",
      " [-0.0840904 ]\n",
      " [-0.52516085]]\n",
      "2 Cost:  1.09 \n",
      "Prediction:\n",
      " [[-0.30894178]\n",
      " [-1.02803135]\n",
      " [-0.57120019]\n",
      " [-0.08175047]\n",
      " [-0.31407332]\n",
      " [-0.42479432]\n",
      " [-0.08406638]\n",
      " [-0.52513695]]\n",
      "3 Cost:  1.08992 \n",
      "Prediction:\n",
      " [[-0.3088907 ]\n",
      " [-1.02798104]\n",
      " [-0.57115871]\n",
      " [-0.08171885]\n",
      " [-0.31403559]\n",
      " [-0.4247579 ]\n",
      " [-0.08404239]\n",
      " [-0.52511305]]\n",
      "4 Cost:  1.08984 \n",
      "Prediction:\n",
      " [[-0.30883956]\n",
      " [-1.02793062]\n",
      " [-0.5711171 ]\n",
      " [-0.08168729]\n",
      " [-0.31399781]\n",
      " [-0.42472142]\n",
      " [-0.08401836]\n",
      " [-0.52508914]]\n",
      "5 Cost:  1.08977 \n",
      "Prediction:\n",
      " [[-0.30878848]\n",
      " [-1.02788031]\n",
      " [-0.57107562]\n",
      " [-0.08165567]\n",
      " [-0.31395996]\n",
      " [-0.42468488]\n",
      " [-0.08399437]\n",
      " [-0.5250653 ]]\n",
      "6 Cost:  1.08969 \n",
      "Prediction:\n",
      " [[-0.30873737]\n",
      " [-1.02783   ]\n",
      " [-0.57103413]\n",
      " [-0.08162411]\n",
      " [-0.31392226]\n",
      " [-0.42464843]\n",
      " [-0.08397035]\n",
      " [-0.5250414 ]]\n",
      "7 Cost:  1.08961 \n",
      "Prediction:\n",
      " [[-0.30868626]\n",
      " [-1.02777958]\n",
      " [-0.57099253]\n",
      " [-0.08159255]\n",
      " [-0.31388444]\n",
      " [-0.42461193]\n",
      " [-0.08394637]\n",
      " [-0.5250175 ]]\n",
      "8 Cost:  1.08953 \n",
      "Prediction:\n",
      " [[-0.30863515]\n",
      " [-1.02772927]\n",
      " [-0.57095104]\n",
      " [-0.08156094]\n",
      " [-0.31384662]\n",
      " [-0.42457548]\n",
      " [-0.08392236]\n",
      " [-0.5249936 ]]\n",
      "9 Cost:  1.08946 \n",
      "Prediction:\n",
      " [[-0.30858403]\n",
      " [-1.02767897]\n",
      " [-0.57090962]\n",
      " [-0.08152938]\n",
      " [-0.31380892]\n",
      " [-0.42453897]\n",
      " [-0.08389837]\n",
      " [-0.5249697 ]]\n",
      "10 Cost:  1.08938 \n",
      "Prediction:\n",
      " [[-0.30853292]\n",
      " [-1.02762866]\n",
      " [-0.57086802]\n",
      " [-0.08149782]\n",
      " [-0.3137711 ]\n",
      " [-0.42450246]\n",
      " [-0.08387434]\n",
      " [-0.52494586]]\n",
      "11 Cost:  1.0893 \n",
      "Prediction:\n",
      " [[-0.30848181]\n",
      " [-1.02757847]\n",
      " [-0.57082653]\n",
      " [-0.08146627]\n",
      " [-0.31373322]\n",
      " [-0.42446601]\n",
      " [-0.08385037]\n",
      " [-0.52492195]]\n",
      "12 Cost:  1.08923 \n",
      "Prediction:\n",
      " [[-0.30843073]\n",
      " [-1.02752793]\n",
      " [-0.57078505]\n",
      " [-0.08143459]\n",
      " [-0.31369549]\n",
      " [-0.42442948]\n",
      " [-0.08382638]\n",
      " [-0.52489811]]\n",
      "13 Cost:  1.08915 \n",
      "Prediction:\n",
      " [[-0.30837962]\n",
      " [-1.02747762]\n",
      " [-0.57074344]\n",
      " [-0.08140305]\n",
      " [-0.31365779]\n",
      " [-0.42439303]\n",
      " [-0.08380237]\n",
      " [-0.52487421]]\n",
      "14 Cost:  1.08907 \n",
      "Prediction:\n",
      " [[-0.30832851]\n",
      " [-1.02742743]\n",
      " [-0.57070196]\n",
      " [-0.08137149]\n",
      " [-0.31361991]\n",
      " [-0.42435652]\n",
      " [-0.08377835]\n",
      " [-0.52485037]]\n",
      "15 Cost:  1.08899 \n",
      "Prediction:\n",
      " [[-0.3082774 ]\n",
      " [-1.02737713]\n",
      " [-0.57066053]\n",
      " [-0.08133988]\n",
      " [-0.31358221]\n",
      " [-0.42432007]\n",
      " [-0.08375441]\n",
      " [-0.52482647]]\n",
      "16 Cost:  1.08892 \n",
      "Prediction:\n",
      " [[-0.30822632]\n",
      " [-1.0273267 ]\n",
      " [-0.57061899]\n",
      " [-0.08130833]\n",
      " [-0.31354442]\n",
      " [-0.42428365]\n",
      " [-0.08373039]\n",
      " [-0.52480257]]\n",
      "17 Cost:  1.08884 \n",
      "Prediction:\n",
      " [[-0.30817521]\n",
      " [-1.0272764 ]\n",
      " [-0.57057744]\n",
      " [-0.08127677]\n",
      " [-0.3135066 ]\n",
      " [-0.42424715]\n",
      " [-0.08370641]\n",
      " [-0.52477872]]\n",
      "18 Cost:  1.08876 \n",
      "Prediction:\n",
      " [[-0.30812413]\n",
      " [-1.02722609]\n",
      " [-0.57053602]\n",
      " [-0.08124517]\n",
      " [-0.31346887]\n",
      " [-0.42421067]\n",
      " [-0.0836824 ]\n",
      " [-0.52475482]]\n",
      "19 Cost:  1.08869 \n",
      "Prediction:\n",
      " [[-0.30807301]\n",
      " [-1.02717578]\n",
      " [-0.57049435]\n",
      " [-0.08121362]\n",
      " [-0.31343111]\n",
      " [-0.42417416]\n",
      " [-0.08365843]\n",
      " [-0.52473092]]\n",
      "20 Cost:  1.08861 \n",
      "Prediction:\n",
      " [[-0.3080219 ]\n",
      " [-1.02712536]\n",
      " [-0.57045305]\n",
      " [-0.08118201]\n",
      " [-0.31339329]\n",
      " [-0.42413765]\n",
      " [-0.08363441]\n",
      " [-0.52470708]]\n",
      "21 Cost:  1.08853 \n",
      "Prediction:\n",
      " [[-0.30797082]\n",
      " [-1.02707505]\n",
      " [-0.57041144]\n",
      " [-0.08115046]\n",
      " [-0.31335551]\n",
      " [-0.42410123]\n",
      " [-0.08361044]\n",
      " [-0.52468324]]\n",
      "22 Cost:  1.08845 \n",
      "Prediction:\n",
      " [[-0.30791971]\n",
      " [-1.02702475]\n",
      " [-0.57037002]\n",
      " [-0.08111886]\n",
      " [-0.31331781]\n",
      " [-0.42406473]\n",
      " [-0.08358643]\n",
      " [-0.5246594 ]]\n",
      "23 Cost:  1.08838 \n",
      "Prediction:\n",
      " [[-0.30786863]\n",
      " [-1.02697456]\n",
      " [-0.57032835]\n",
      " [-0.08108731]\n",
      " [-0.31327996]\n",
      " [-0.42402831]\n",
      " [-0.08356246]\n",
      " [-0.52463543]]\n",
      "24 Cost:  1.0883 \n",
      "Prediction:\n",
      " [[-0.30781752]\n",
      " [-1.02692401]\n",
      " [-0.57028693]\n",
      " [-0.08105577]\n",
      " [-0.3132422 ]\n",
      " [-0.42399168]\n",
      " [-0.08353845]\n",
      " [-0.52461165]]\n",
      "25 Cost:  1.08822 \n",
      "Prediction:\n",
      " [[-0.30776644]\n",
      " [-1.02687371]\n",
      " [-0.57024544]\n",
      " [-0.08102416]\n",
      " [-0.31320441]\n",
      " [-0.42395526]\n",
      " [-0.0835145 ]\n",
      " [-0.52458775]]\n",
      "26 Cost:  1.08815 \n",
      "Prediction:\n",
      " [[-0.30771533]\n",
      " [-1.02682352]\n",
      " [-0.57020402]\n",
      " [-0.08099256]\n",
      " [-0.31316665]\n",
      " [-0.42391875]\n",
      " [-0.0834905 ]\n",
      " [-0.52456391]]\n",
      "27 Cost:  1.08807 \n",
      "Prediction:\n",
      " [[-0.30766425]\n",
      " [-1.02677321]\n",
      " [-0.57016242]\n",
      " [-0.08096108]\n",
      " [-0.31312886]\n",
      " [-0.42388234]\n",
      " [-0.0834665 ]\n",
      " [-0.52454001]]\n",
      "28 Cost:  1.08799 \n",
      "Prediction:\n",
      " [[-0.30761313]\n",
      " [-1.02672291]\n",
      " [-0.57012093]\n",
      " [-0.08092941]\n",
      " [-0.31309116]\n",
      " [-0.42384589]\n",
      " [-0.08344252]\n",
      " [-0.52451617]]\n",
      "29 Cost:  1.08791 \n",
      "Prediction:\n",
      " [[-0.30756205]\n",
      " [-1.02667248]\n",
      " [-0.57007951]\n",
      " [-0.08089787]\n",
      " [-0.31305337]\n",
      " [-0.42380941]\n",
      " [-0.08341855]\n",
      " [-0.52449226]]\n",
      "30 Cost:  1.08784 \n",
      "Prediction:\n",
      " [[-0.30751097]\n",
      " [-1.02662218]\n",
      " [-0.5700379 ]\n",
      " [-0.08086633]\n",
      " [-0.31301558]\n",
      " [-0.42377299]\n",
      " [-0.08339455]\n",
      " [-0.52446836]]\n",
      "31 Cost:  1.08776 \n",
      "Prediction:\n",
      " [[-0.30745986]\n",
      " [-1.02657199]\n",
      " [-0.56999648]\n",
      " [-0.08083473]\n",
      " [-0.31297782]\n",
      " [-0.42373636]\n",
      " [-0.08337058]\n",
      " [-0.52444452]]\n",
      "32 Cost:  1.08768 \n",
      "Prediction:\n",
      " [[-0.30740878]\n",
      " [-1.02652156]\n",
      " [-0.56995499]\n",
      " [-0.08080319]\n",
      " [-0.31294003]\n",
      " [-0.42369995]\n",
      " [-0.08334658]\n",
      " [-0.52442068]]\n",
      "33 Cost:  1.08761 \n",
      "Prediction:\n",
      " [[-0.3073577 ]\n",
      " [-1.02647126]\n",
      " [-0.56991351]\n",
      " [-0.08077165]\n",
      " [-0.3129023 ]\n",
      " [-0.42366353]\n",
      " [-0.08332261]\n",
      " [-0.52439678]]\n",
      "34 Cost:  1.08753 \n",
      "Prediction:\n",
      " [[-0.30730662]\n",
      " [-1.02642095]\n",
      " [-0.5698719 ]\n",
      " [-0.08074005]\n",
      " [-0.31286457]\n",
      " [-0.42362705]\n",
      " [-0.08329861]\n",
      " [-0.52437294]]\n",
      "35 Cost:  1.08745 \n",
      "Prediction:\n",
      " [[-0.30725551]\n",
      " [-1.02637076]\n",
      " [-0.56983048]\n",
      " [-0.08070846]\n",
      " [-0.31282669]\n",
      " [-0.4235906 ]\n",
      " [-0.08327465]\n",
      " [-0.52434909]]\n",
      "36 Cost:  1.08738 \n",
      "Prediction:\n",
      " [[-0.30720443]\n",
      " [-1.02632046]\n",
      " [-0.56978893]\n",
      " [-0.08067691]\n",
      " [-0.31278902]\n",
      " [-0.42355412]\n",
      " [-0.08325064]\n",
      " [-0.52432519]]\n",
      "37 Cost:  1.0873 \n",
      "Prediction:\n",
      " [[-0.30715334]\n",
      " [-1.02626991]\n",
      " [-0.56974757]\n",
      " [-0.08064538]\n",
      " [-0.31275123]\n",
      " [-0.4235177 ]\n",
      " [-0.08322668]\n",
      " [-0.52430135]]\n",
      "38 Cost:  1.08722 \n",
      "Prediction:\n",
      " [[-0.30710226]\n",
      " [-1.02621973]\n",
      " [-0.56970602]\n",
      " [-0.08061379]\n",
      " [-0.3127135 ]\n",
      " [-0.42348117]\n",
      " [-0.08320269]\n",
      " [-0.52427745]]\n",
      "39 Cost:  1.08714 \n",
      "Prediction:\n",
      " [[-0.30705118]\n",
      " [-1.02616942]\n",
      " [-0.56966442]\n",
      " [-0.08058225]\n",
      " [-0.31267565]\n",
      " [-0.42344469]\n",
      " [-0.08317872]\n",
      " [-0.52425361]]\n",
      "40 Cost:  1.08707 \n",
      "Prediction:\n",
      " [[-0.3070001 ]\n",
      " [-1.02611923]\n",
      " [-0.56962299]\n",
      " [-0.08055072]\n",
      " [-0.31263798]\n",
      " [-0.42340827]\n",
      " [-0.08315475]\n",
      " [-0.52422976]]\n",
      "41 Cost:  1.08699 \n",
      "Prediction:\n",
      " [[-0.30694902]\n",
      " [-1.02606881]\n",
      " [-0.56958151]\n",
      " [-0.08051918]\n",
      " [-0.3126002 ]\n",
      " [-0.42337179]\n",
      " [-0.08313076]\n",
      " [-0.52420592]]\n",
      "42 Cost:  1.08691 \n",
      "Prediction:\n",
      " [[-0.30689794]\n",
      " [-1.0260185 ]\n",
      " [-0.56953996]\n",
      " [-0.08048753]\n",
      " [-0.31256247]\n",
      " [-0.42333537]\n",
      " [-0.08310677]\n",
      " [-0.52418202]]\n",
      "43 Cost:  1.08684 \n",
      "Prediction:\n",
      " [[-0.30684686]\n",
      " [-1.02596819]\n",
      " [-0.56949854]\n",
      " [-0.080456  ]\n",
      " [-0.31252474]\n",
      " [-0.4232989 ]\n",
      " [-0.08308284]\n",
      " [-0.52415818]]\n",
      "44 Cost:  1.08676 \n",
      "Prediction:\n",
      " [[-0.30679578]\n",
      " [-1.02591789]\n",
      " [-0.56945705]\n",
      " [-0.08042447]\n",
      " [-0.31248695]\n",
      " [-0.42326248]\n",
      " [-0.08305885]\n",
      " [-0.52413434]]\n",
      "45 Cost:  1.08668 \n",
      "Prediction:\n",
      " [[-0.30674469]\n",
      " [-1.02586746]\n",
      " [-0.56941551]\n",
      " [-0.08039288]\n",
      " [-0.31244916]\n",
      " [-0.42322594]\n",
      " [-0.08303486]\n",
      " [-0.5241105 ]]\n",
      "46 Cost:  1.0866 \n",
      "Prediction:\n",
      " [[-0.30669361]\n",
      " [-1.02581728]\n",
      " [-0.56937402]\n",
      " [-0.08036135]\n",
      " [-0.31241143]\n",
      " [-0.42318946]\n",
      " [-0.0830109 ]\n",
      " [-0.52408665]]\n",
      "47 Cost:  1.08653 \n",
      "Prediction:\n",
      " [[-0.30664253]\n",
      " [-1.02576697]\n",
      " [-0.5693326 ]\n",
      " [-0.08032982]\n",
      " [-0.3123737 ]\n",
      " [-0.42315304]\n",
      " [-0.08298694]\n",
      " [-0.52406281]]\n",
      "48 Cost:  1.08645 \n",
      "Prediction:\n",
      " [[-0.30659145]\n",
      " [-1.02571678]\n",
      " [-0.569291  ]\n",
      " [-0.08029824]\n",
      " [-0.31233591]\n",
      " [-0.42311651]\n",
      " [-0.08296295]\n",
      " [-0.52403891]]\n",
      "49 Cost:  1.08637 \n",
      "Prediction:\n",
      " [[-0.30654037]\n",
      " [-1.02566624]\n",
      " [-0.56924969]\n",
      " [-0.08026671]\n",
      " [-0.31229818]\n",
      " [-0.42308009]\n",
      " [-0.08293897]\n",
      " [-0.52401507]]\n",
      "50 Cost:  1.0863 \n",
      "Prediction:\n",
      " [[-0.30648929]\n",
      " [-1.02561593]\n",
      " [-0.56920809]\n",
      " [-0.08023512]\n",
      " [-0.31226045]\n",
      " [-0.42304367]\n",
      " [-0.08291501]\n",
      " [-0.52399123]]\n",
      "51 Cost:  1.08622 \n",
      "Prediction:\n",
      " [[-0.30643821]\n",
      " [-1.02556574]\n",
      " [-0.56916666]\n",
      " [-0.08020359]\n",
      " [-0.3122226 ]\n",
      " [-0.42300713]\n",
      " [-0.08289102]\n",
      " [-0.52396739]]\n",
      "52 Cost:  1.08614 \n",
      "Prediction:\n",
      " [[-0.30638713]\n",
      " [-1.02551556]\n",
      " [-0.56912524]\n",
      " [-0.08017202]\n",
      " [-0.31218487]\n",
      " [-0.42297071]\n",
      " [-0.08286707]\n",
      " [-0.52394354]]\n",
      "53 Cost:  1.08607 \n",
      "Prediction:\n",
      " [[-0.30633608]\n",
      " [-1.02546513]\n",
      " [-0.56908363]\n",
      " [-0.08014049]\n",
      " [-0.31214711]\n",
      " [-0.42293426]\n",
      " [-0.08284311]\n",
      " [-0.52391964]]\n",
      "54 Cost:  1.08599 \n",
      "Prediction:\n",
      " [[-0.30628499]\n",
      " [-1.02541482]\n",
      " [-0.56904221]\n",
      " [-0.08010897]\n",
      " [-0.31210938]\n",
      " [-0.42289779]\n",
      " [-0.08281913]\n",
      " [-0.5238958 ]]\n",
      "55 Cost:  1.08591 \n",
      "Prediction:\n",
      " [[-0.30623391]\n",
      " [-1.02536452]\n",
      " [-0.56900078]\n",
      " [-0.08007738]\n",
      " [-0.31207171]\n",
      " [-0.42286137]\n",
      " [-0.08279514]\n",
      " [-0.52387196]]\n",
      "56 Cost:  1.08583 \n",
      "Prediction:\n",
      " [[-0.30618283]\n",
      " [-1.02531433]\n",
      " [-0.56895924]\n",
      " [-0.08004586]\n",
      " [-0.31203386]\n",
      " [-0.42282489]\n",
      " [-0.08277123]\n",
      " [-0.52384812]]\n",
      "57 Cost:  1.08576 \n",
      "Prediction:\n",
      " [[-0.30613178]\n",
      " [-1.02526391]\n",
      " [-0.56891775]\n",
      " [-0.08001428]\n",
      " [-0.3119961 ]\n",
      " [-0.4227885 ]\n",
      " [-0.08274724]\n",
      " [-0.52382427]]\n",
      "58 Cost:  1.08568 \n",
      "Prediction:\n",
      " [[-0.3060807 ]\n",
      " [-1.0252136 ]\n",
      " [-0.56887633]\n",
      " [-0.07998282]\n",
      " [-0.31195837]\n",
      " [-0.42275202]\n",
      " [-0.08272329]\n",
      " [-0.52380043]]\n",
      "59 Cost:  1.0856 \n",
      "Prediction:\n",
      " [[-0.30602962]\n",
      " [-1.02516341]\n",
      " [-0.56883472]\n",
      " [-0.07995118]\n",
      " [-0.31192064]\n",
      " [-0.42271554]\n",
      " [-0.08269931]\n",
      " [-0.52377659]]\n",
      "60 Cost:  1.08553 \n",
      "Prediction:\n",
      " [[-0.30597857]\n",
      " [-1.02511311]\n",
      " [-0.5687933 ]\n",
      " [-0.07991967]\n",
      " [-0.31188282]\n",
      " [-0.4226791 ]\n",
      " [-0.08267537]\n",
      " [-0.52375275]]\n",
      "61 Cost:  1.08545 \n",
      "Prediction:\n",
      " [[-0.30592749]\n",
      " [-1.02506292]\n",
      " [-0.56875193]\n",
      " [-0.0798882 ]\n",
      " [-0.31184515]\n",
      " [-0.42264268]\n",
      " [-0.08265138]\n",
      " [-0.52372891]]\n",
      "62 Cost:  1.08537 \n",
      "Prediction:\n",
      " [[-0.30587643]\n",
      " [-1.02501237]\n",
      " [-0.56871033]\n",
      " [-0.07985657]\n",
      " [-0.31180745]\n",
      " [-0.42260623]\n",
      " [-0.08262744]\n",
      " [-0.52370507]]\n",
      "63 Cost:  1.0853 \n",
      "Prediction:\n",
      " [[-0.30582535]\n",
      " [-1.02496219]\n",
      " [-0.5686689 ]\n",
      " [-0.07982505]\n",
      " [-0.31176966]\n",
      " [-0.42256975]\n",
      " [-0.08260346]\n",
      " [-0.52368122]]\n",
      "64 Cost:  1.08522 \n",
      "Prediction:\n",
      " [[-0.3057743 ]\n",
      " [-1.024912  ]\n",
      " [-0.56862742]\n",
      " [-0.07979348]\n",
      " [-0.31173196]\n",
      " [-0.42253336]\n",
      " [-0.08257952]\n",
      " [-0.52365738]]\n",
      "65 Cost:  1.08514 \n",
      "Prediction:\n",
      " [[-0.30572322]\n",
      " [-1.02486169]\n",
      " [-0.56858593]\n",
      " [-0.0797619 ]\n",
      " [-0.31169418]\n",
      " [-0.42249689]\n",
      " [-0.08255554]\n",
      " [-0.52363354]]\n",
      "66 Cost:  1.08507 \n",
      "Prediction:\n",
      " [[-0.30567217]\n",
      " [-1.02481139]\n",
      " [-0.56854457]\n",
      " [-0.07973044]\n",
      " [-0.31165642]\n",
      " [-0.4224605 ]\n",
      " [-0.08253159]\n",
      " [-0.5236097 ]]\n",
      "67 Cost:  1.08499 \n",
      "Prediction:\n",
      " [[-0.30562109]\n",
      " [-1.02476108]\n",
      " [-0.56850302]\n",
      " [-0.07969888]\n",
      " [-0.31161869]\n",
      " [-0.42242402]\n",
      " [-0.08250763]\n",
      " [-0.52358586]]\n",
      "68 Cost:  1.08491 \n",
      "Prediction:\n",
      " [[-0.30557016]\n",
      " [-1.02471077]\n",
      " [-0.56846154]\n",
      " [-0.07966736]\n",
      " [-0.31158105]\n",
      " [-0.42238757]\n",
      " [-0.08248368]\n",
      " [-0.52356207]]\n",
      "69 Cost:  1.08483 \n",
      "Prediction:\n",
      " [[-0.3055191 ]\n",
      " [-1.02466059]\n",
      " [-0.56842017]\n",
      " [-0.07963578]\n",
      " [-0.31154323]\n",
      " [-0.42235118]\n",
      " [-0.0824597 ]\n",
      " [-0.52353817]]\n",
      "70 Cost:  1.08476 \n",
      "Prediction:\n",
      " [[-0.30546802]\n",
      " [-1.0246104 ]\n",
      " [-0.56837875]\n",
      " [-0.07960434]\n",
      " [-0.31150556]\n",
      " [-0.4223147 ]\n",
      " [-0.08243576]\n",
      " [-0.52351439]]\n",
      "71 Cost:  1.08468 \n",
      "Prediction:\n",
      " [[-0.30541709]\n",
      " [-1.02456009]\n",
      " [-0.56833726]\n",
      " [-0.07957283]\n",
      " [-0.31146786]\n",
      " [-0.42227831]\n",
      " [-0.08241183]\n",
      " [-0.52349049]]\n",
      "72 Cost:  1.0846 \n",
      "Prediction:\n",
      " [[-0.30536616]\n",
      " [-1.02450991]\n",
      " [-0.5682959 ]\n",
      " [-0.07954131]\n",
      " [-0.31143016]\n",
      " [-0.42224193]\n",
      " [-0.08238788]\n",
      " [-0.52346671]]\n",
      "73 Cost:  1.08453 \n",
      "Prediction:\n",
      " [[-0.3053152 ]\n",
      " [-1.02445972]\n",
      " [-0.56825441]\n",
      " [-0.07950981]\n",
      " [-0.31139249]\n",
      " [-0.42220557]\n",
      " [-0.08236392]\n",
      " [-0.52344286]]\n",
      "74 Cost:  1.08445 \n",
      "Prediction:\n",
      " [[-0.30526426]\n",
      " [-1.02440941]\n",
      " [-0.56821305]\n",
      " [-0.07947829]\n",
      " [-0.31135479]\n",
      " [-0.42216918]\n",
      " [-0.08234   ]\n",
      " [-0.52341902]]\n",
      "75 Cost:  1.08437 \n",
      "Prediction:\n",
      " [[-0.30521321]\n",
      " [-1.02435923]\n",
      " [-0.56817156]\n",
      " [-0.07944679]\n",
      " [-0.31131709]\n",
      " [-0.42213273]\n",
      " [-0.08231604]\n",
      " [-0.52339518]]\n",
      "76 Cost:  1.0843 \n",
      "Prediction:\n",
      " [[-0.30516228]\n",
      " [-1.02430904]\n",
      " [-0.56813025]\n",
      " [-0.07941528]\n",
      " [-0.31127939]\n",
      " [-0.42209634]\n",
      " [-0.08229212]\n",
      " [-0.52337134]]\n",
      "77 Cost:  1.08422 \n",
      "Prediction:\n",
      " [[-0.30511135]\n",
      " [-1.02425873]\n",
      " [-0.56808871]\n",
      " [-0.07938384]\n",
      " [-0.31124175]\n",
      " [-0.42205989]\n",
      " [-0.08226816]\n",
      " [-0.52334756]]\n",
      "78 Cost:  1.08414 \n",
      "Prediction:\n",
      " [[-0.30506027]\n",
      " [-1.02420855]\n",
      " [-0.56804729]\n",
      " [-0.07935233]\n",
      " [-0.31120396]\n",
      " [-0.42202359]\n",
      " [-0.08224423]\n",
      " [-0.52332371]]\n",
      "79 Cost:  1.08407 \n",
      "Prediction:\n",
      " [[-0.30500934]\n",
      " [-1.02415836]\n",
      " [-0.56800604]\n",
      " [-0.07932077]\n",
      " [-0.31116632]\n",
      " [-0.42198721]\n",
      " [-0.08222029]\n",
      " [-0.52329987]]\n",
      "80 Cost:  1.08399 \n",
      "Prediction:\n",
      " [[-0.3049584 ]\n",
      " [-1.02410817]\n",
      " [-0.56796443]\n",
      " [-0.07928926]\n",
      " [-0.31112868]\n",
      " [-0.42195076]\n",
      " [-0.08219635]\n",
      " [-0.52327603]]\n",
      "81 Cost:  1.08391 \n",
      "Prediction:\n",
      " [[-0.30490747]\n",
      " [-1.02405787]\n",
      " [-0.56792319]\n",
      " [-0.07925776]\n",
      " [-0.31109092]\n",
      " [-0.42191437]\n",
      " [-0.08217239]\n",
      " [-0.52325225]]\n",
      "82 Cost:  1.08384 \n",
      "Prediction:\n",
      " [[-0.30485654]\n",
      " [-1.02400768]\n",
      " [-0.5678817 ]\n",
      " [-0.07922626]\n",
      " [-0.31105328]\n",
      " [-0.42187792]\n",
      " [-0.08214846]\n",
      " [-0.52322841]]\n",
      "83 Cost:  1.08376 \n",
      "Prediction:\n",
      " [[-0.30480549]\n",
      " [-1.02395749]\n",
      " [-0.56784034]\n",
      " [-0.07919475]\n",
      " [-0.31101558]\n",
      " [-0.42184165]\n",
      " [-0.0821245 ]\n",
      " [-0.52320457]]\n",
      "84 Cost:  1.08368 \n",
      "Prediction:\n",
      " [[-0.30475456]\n",
      " [-1.02390718]\n",
      " [-0.56779885]\n",
      " [-0.07916331]\n",
      " [-0.31097788]\n",
      " [-0.42180526]\n",
      " [-0.0821006 ]\n",
      " [-0.52318072]]\n",
      "85 Cost:  1.08361 \n",
      "Prediction:\n",
      " [[-0.30470362]\n",
      " [-1.023857  ]\n",
      " [-0.56775749]\n",
      " [-0.07913181]\n",
      " [-0.31094018]\n",
      " [-0.42176881]\n",
      " [-0.08207664]\n",
      " [-0.52315688]]\n",
      "86 Cost:  1.08353 \n",
      "Prediction:\n",
      " [[-0.30465257]\n",
      " [-1.02380681]\n",
      " [-0.56771606]\n",
      " [-0.07910031]\n",
      " [-0.31090254]\n",
      " [-0.42173243]\n",
      " [-0.08205274]\n",
      " [-0.5231331 ]]\n",
      "87 Cost:  1.08345 \n",
      "Prediction:\n",
      " [[-0.30460164]\n",
      " [-1.02375662]\n",
      " [-0.56767464]\n",
      " [-0.07906881]\n",
      " [-0.31086484]\n",
      " [-0.42169598]\n",
      " [-0.08202878]\n",
      " [-0.52310932]]\n",
      "88 Cost:  1.08338 \n",
      "Prediction:\n",
      " [[-0.30455071]\n",
      " [-1.02370644]\n",
      " [-0.56763321]\n",
      " [-0.07903738]\n",
      " [-0.3108272 ]\n",
      " [-0.42165971]\n",
      " [-0.08200485]\n",
      " [-0.52308547]]\n",
      "89 Cost:  1.0833 \n",
      "Prediction:\n",
      " [[-0.30449978]\n",
      " [-1.02365625]\n",
      " [-0.56759191]\n",
      " [-0.07900582]\n",
      " [-0.3107895 ]\n",
      " [-0.42162332]\n",
      " [-0.0819809 ]\n",
      " [-0.52306163]]\n",
      "90 Cost:  1.08322 \n",
      "Prediction:\n",
      " [[-0.30444884]\n",
      " [-1.02360594]\n",
      " [-0.56755036]\n",
      " [-0.07897432]\n",
      " [-0.31075174]\n",
      " [-0.42158687]\n",
      " [-0.08195697]\n",
      " [-0.52303779]]\n",
      "91 Cost:  1.08315 \n",
      "Prediction:\n",
      " [[-0.30439782]\n",
      " [-1.02355576]\n",
      " [-0.56750906]\n",
      " [-0.07894282]\n",
      " [-0.31071413]\n",
      " [-0.42155051]\n",
      " [-0.08193304]\n",
      " [-0.52301401]]\n",
      "92 Cost:  1.08307 \n",
      "Prediction:\n",
      " [[-0.30434689]\n",
      " [-1.02350569]\n",
      " [-0.56746751]\n",
      " [-0.07891133]\n",
      " [-0.31067649]\n",
      " [-0.42151406]\n",
      " [-0.08190912]\n",
      " [-0.52299017]]\n",
      "93 Cost:  1.08299 \n",
      "Prediction:\n",
      " [[-0.30429596]\n",
      " [-1.0234555 ]\n",
      " [-0.5674262 ]\n",
      " [-0.07887983]\n",
      " [-0.31063879]\n",
      " [-0.42147774]\n",
      " [-0.08188519]\n",
      " [-0.52296638]]\n",
      "94 Cost:  1.08291 \n",
      "Prediction:\n",
      " [[-0.30424491]\n",
      " [-1.02340519]\n",
      " [-0.56738472]\n",
      " [-0.07884841]\n",
      " [-0.31060109]\n",
      " [-0.42144135]\n",
      " [-0.08186127]\n",
      " [-0.52294254]]\n",
      "95 Cost:  1.08284 \n",
      "Prediction:\n",
      " [[-0.304194  ]\n",
      " [-1.02335489]\n",
      " [-0.56734341]\n",
      " [-0.07881685]\n",
      " [-0.31056342]\n",
      " [-0.42140499]\n",
      " [-0.08183731]\n",
      " [-0.52291876]]\n",
      "96 Cost:  1.08276 \n",
      "Prediction:\n",
      " [[-0.30414307]\n",
      " [-1.02330494]\n",
      " [-0.56730199]\n",
      " [-0.07878536]\n",
      " [-0.31052577]\n",
      " [-0.4213686 ]\n",
      " [-0.0818134 ]\n",
      " [-0.52289492]]\n",
      "97 Cost:  1.08268 \n",
      "Prediction:\n",
      " [[-0.30409214]\n",
      " [-1.02325463]\n",
      " [-0.56726062]\n",
      " [-0.07875393]\n",
      " [-0.31048802]\n",
      " [-0.42133221]\n",
      " [-0.08178947]\n",
      " [-0.52287114]]\n",
      "98 Cost:  1.08261 \n",
      "Prediction:\n",
      " [[-0.30404124]\n",
      " [-1.02320445]\n",
      " [-0.56721914]\n",
      " [-0.07872243]\n",
      " [-0.31045046]\n",
      " [-0.42129585]\n",
      " [-0.08176555]\n",
      " [-0.52284729]]\n",
      "99 Cost:  1.08253 \n",
      "Prediction:\n",
      " [[-0.3039903 ]\n",
      " [-1.02315438]\n",
      " [-0.56717783]\n",
      " [-0.07869095]\n",
      " [-0.31041276]\n",
      " [-0.42125952]\n",
      " [-0.08174166]\n",
      " [-0.52282345]]\n",
      "100 Cost:  1.08245 \n",
      "Prediction:\n",
      " [[-0.30393949]\n",
      " [-1.02310431]\n",
      " [-0.56713653]\n",
      " [-0.07865958]\n",
      " [-0.31037518]\n",
      " [-0.42122319]\n",
      " [-0.08171774]\n",
      " [-0.52279967]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        cost_val, hy_val, _ = sess.run(\n",
    "            [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "logits = tf.matmul(X,W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is epoch?\n",
    "\n",
    "one epoch = one forward pass and one backend pass of all the training examples\n",
    "\n",
    "batchsize = the number of tarining example in one forward/backward pass\n",
    "\n",
    "number of iterations = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
    "\n",
    "Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 2.778248697\n",
      "Epoch: 0002 cost = 1.056700854\n",
      "Epoch: 0003 cost = 0.852730018\n",
      "Epoch: 0004 cost = 0.749228868\n",
      "Epoch: 0005 cost = 0.682466504\n",
      "Epoch: 0006 cost = 0.634199119\n",
      "Epoch: 0007 cost = 0.597917128\n",
      "Epoch: 0008 cost = 0.569442023\n",
      "Epoch: 0009 cost = 0.546011793\n",
      "Epoch: 0010 cost = 0.526723328\n",
      "Epoch: 0011 cost = 0.509755617\n",
      "Epoch: 0012 cost = 0.495010713\n",
      "Epoch: 0013 cost = 0.481985383\n",
      "Epoch: 0014 cost = 0.470390337\n",
      "Epoch: 0015 cost = 0.460224089\n",
      "Learning finished\n",
      "Accuracy:  0.8917\n",
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff05f404780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap='Greys',\n",
    "        interpolation='nearest')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-py3.5",
   "language": "python",
   "name": "tensorfllow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
